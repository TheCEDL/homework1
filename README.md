# Homework1 - Introduce a New NN with Memory 
In this homework, we will summarize two papers related to NN with memory <br>
The papers include: <br>
1. LSTM: A Search Space Odyssey <br>
2. Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes <br>

負責部份:<br>
1. LSTM: A Search Space Odyssey: <br>
   Organizer:周育潤, 翁慶年 <br>
2. Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes <br>
   Organizer:鄭乃嘉, 賴筱婷 <br>



# Motivation
To have a more comprehensive and deeper understanding about the different variants of <br>
LSTM and the practical mathematical method applied on it, we go throgh two informative <br>
papers to get clear insight of this frontier research territory <br>

* LSTM: A Search Space Odyssey <a href="https://arxiv.org/abs/1503.04069">slides</a>
* Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes <a href="https://arxiv.org/abs/1607.00036">slides</a>

# To-Do
* [+10] Please find a recent paper (2014-2015) which introduced a NN with memory.
* [+50] Write a report to briefly introduce the paper;
* [+40] then, focus on discussing the unique properties of the new NN and where it can be applied to take advantage of the properties.

# Candidates
* Search RNN on Arxiv-sanity <a href="http://www.arxiv-sanity.com/search?q=rnn">link</a>
* Jianpeng Cheng et al. Long Short-Term Memory-Networks for Machine Reading. arXiv16’. 
* Nal Kalchbrenner et al. Grid Long Short-Term Memory. arXiv16’. (From DeepMind, Alex)
* Kaisheng Yao et al. Depth-Gated LSTM. arXiv15’. 
* Shuohang Wang et al. Learning Natural Language Inference with LSTM. arXiv15’. 
* Junyoung Chung et al. Gated Feedback Recurrent Neural Networks. arXiv15’.

# Other
* Due on Oct. 3rd before class.
